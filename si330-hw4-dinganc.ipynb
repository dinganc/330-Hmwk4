{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SI 330: Homework 4: APIs on AWS\n",
    "\n",
    "\n",
    "## Due: Friday, February 9, 2018,  11:59:00pm\n",
    "\n",
    "### Submission instructions</font>\n",
    "After completing this homework, you will turn in two files via Canvas ->  Assignments -> HW 4:\n",
    "Your Notebook, named si330-hw4-YOUR_UNIQUE_NAME.ipynb and\n",
    "the HTML file, named si330-hw4-YOUR_UNIQUE_NAME.html.\n",
    "\n",
    "### Name:  Dingan Chen\n",
    "### Uniqname: dinganc\n",
    "### People you worked with: I worked by myself\n",
    "\n",
    "## Top-Level Goal\n",
    "To create a microservice that returns the counts of all bigrams in a text passage.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Objectives\n",
    "After completing this Lab, you should know how to:\n",
    "* create an AWS Lambda function that takes a string and returns the counts of all bigrams in that text\n",
    "* write an AWS API Gateway integration which allows both GET and POST requests to access an AWS Lambda\n",
    "* write documenation to the microservice that you've created\n",
    "\n",
    "### Note: See end of notebook for notes about going \"Above and Beyond\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Steps For Analysis\n",
    "Here's an overview of the steps that you'll need to do to complete this lab.\n",
    "2. Upload data to an S3 bucket\n",
    "1. Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text, both via a POST request with the text and via a GET request to a URL that returns the text (e.g. an S3 bucket)\n",
    "3. Create a python code block in this notebook to demonstrate the functionality of your microservice\n",
    "\n",
    "Each of these steps is detailed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload data to an S3 bucket\n",
    "To get ready to test the POST functionality of the code you generate in the next step, you should upload a text file that is **500 or fewer lines** to an S3 bucket.  See the description of CORS for an explanation of why we want to put the data in the same domain (amazonaws.com) as the Lambda.\n",
    "\n",
    "Follow the same approach that we used in the lab to upload a small text file to your S3 bucket, ensuring that the permissions are set to allow public access\n",
    "\n",
    "### <font color=\"magenta\">Q1: Enter the URL of your text file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://s3.us-east-2.amazonaws.com/bucket-for-si330-hmwk4-dinganc/PrideAndPrejudice.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create an AWS Lambda function that normalizes, tokenizes, and creates and counts bigrams from text\n",
    "\n",
    "Similar to what we did in the lab, you're going to create a microservice that consists of two parts: an AWS Lambda and an API Gateway.  You can use exactly the same technique that we did in the lab to get started.\n",
    "\n",
    "You will need to modify the code in the Lambda to handle two types of requests:\n",
    "1. A GET request with a queryStringParameter of url=http://some.url.goes.here/text.txt, which specifies the location of the text to be processed and\n",
    "2. A POST request with the text to be processed included as the \"text\" value in the body payload.\n",
    "\n",
    "### The following code block is a reasonable starting point for creating your Lambda.  Note that this code should not be run in this notebook but rather serve as the starting point for your work in the Lambda editor.\n",
    "\n",
    "**NOTE** Please see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python for hints about how to create bigrams without NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PUT SOME DOCUMENTATION HERE\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests # This line has been added. \n",
    "# You'll need to figure out how to use this requests, \n",
    "# but it works the same way as the requests module (called using ```import requests```) in python.\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"text\": \"\"}\n",
    "    # Handle GET method\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            url = ... # retrieve the text from the URL\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            pass\n",
    "    # normalize\n",
    "    # tokenize\n",
    "    # find bigrams\n",
    "    # NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "    #       for hints about how to create bigrams\n",
    "    # count bigrams\n",
    "    \n",
    "    # Note the strict format of the return dictionary\n",
    "    # It must contain these three elements, and the body\n",
    "    # must be a stringified JSON object (i.e. you have to call \n",
    "    # json.dumps on the JSON structure you're returning)\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2a: Enter the URL of your Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/FromTXTToBigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"magenta\">Q2b: Copy your final Lambda code into the following code block (but do not run it here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This API takes in an url to a text file and returns the ngram count as a json formatted string\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "from collections import defaultdict\n",
    "# normalize: convert the text to lowercase\n",
    "def get_text_and_normalize(url):\n",
    "    r=requests.get(url)\n",
    "    r.encoding='utf-8'\n",
    "    text=r.text.lower().strip()\n",
    "    text=re.sub(r'\\ufeff','',text)\n",
    "    text=re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "# tokenize: split the text into sentences, the split each sentence into words\n",
    "# NOTE: it's probably best to use re.split()\n",
    "def tokenize(text):\n",
    "    sentences=[re.sub(r'\\r\\n',' ',i) for i in re.split(r'\\r\\n\\r\\n',text)if i !='']\n",
    "    return sentences\n",
    "\n",
    "# find bigrams\n",
    "# NOTE: it's very difficult to set up NLTK on Lambda, so you'll need to find bigrams \"manually\"\n",
    "# NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "#       for hints about how to create bigrams\n",
    "    \n",
    "# count bigrams\n",
    "    \n",
    "# Note the strict format of the return dictionary\n",
    "# It must contain these three elements, and the body\n",
    "# must be a stringified JSON object (i.e. you have to call \n",
    "# json.dumps on the JSON structure you're returning)\n",
    "\n",
    "def bigram_count(sentences):\n",
    "    bigrams=[]\n",
    "    bigram_k=defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words)):\n",
    "            if i < len(words) - 1:\n",
    "                bigrams.append((words[i], words[i + 1]))\n",
    "    for bigram in bigrams:\n",
    "        bigram_k[bigram]+=1\n",
    "    return sorted(bigram_k.items(),key=lambda x: x[1],reverse=True)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"bigrams\": \"\"}\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            url = params['url'] # retrieve the text from the URL\n",
    "            try:\n",
    "                d['bigrams']=bigram_count(tokenize(get_text_and_normalize(url)))\n",
    "            except:\n",
    "                d['error']='error in parsing linked text file'\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'url' in body:\n",
    "            url=body['url']\n",
    "            try:\n",
    "                d['bigrams']=bigram_count(tokenize(get_text_and_normalize(url)))\n",
    "            except:\n",
    "                d['error']='error in parsing linked text file'\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Demonstrate the GET and POST functionality of your Lambda\n",
    "\n",
    "### <font color=\"magenta\">Q3: Create a code block that uses `requests` to demonstrate the functionality of your Lambda.  You can modify the template below or create your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing top 5, with GET method\n",
      "\n",
      "\n",
      "( that , he ), which appears 5  times\n",
      "\n",
      "\n",
      "( my , dear ), which appears 4  times\n",
      "\n",
      "\n",
      "( mr , bennet ), which appears 4  times\n",
      "\n",
      "\n",
      "( it , is ), which appears 2  times\n",
      "\n",
      "\n",
      "( a , single ), which appears 2  times\n",
      "\n",
      "printing top 5, with POST method\n",
      "\n",
      "\n",
      "( that , he ), which appears 5  times\n",
      "\n",
      "\n",
      "( my , dear ), which appears 4  times\n",
      "\n",
      "\n",
      "( mr , bennet ), which appears 4  times\n",
      "\n",
      "\n",
      "( it , is ), which appears 2  times\n",
      "\n",
      "\n",
      "( a , single ), which appears 2  times\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/FromTXTToBigrams' # change this URL\n",
    "textURL = 'https://s3.us-east-2.amazonaws.com/bucket-for-si330-hmwk4-dinganc/PrideAndPrejudice.txt' # change this URL\n",
    "\n",
    "# Demonstrate the GET functionality by passing the URL of your text file in S3 to your Lambda as a GET request\n",
    "response = requests.get(lambdaURL + '?url=' + textURL)\n",
    "bigrams = json.loads(response.text)\n",
    "\n",
    "print ('printing top 5, with GET method')\n",
    "\n",
    "for pair in bigrams['bigrams'][:5]:\n",
    "    print('\\n')\n",
    "    print(\"(\",pair[0][0],\",\",pair[0][1],\"), which appears\",pair[1],\" times\")# you should make this print something nicer\n",
    "\n",
    "# Demonstrate the POST functionality by passing the text as a JSON parameter to requests.post()\n",
    "# note that we retrieve the contents of the S3 bucket using requests.get()\n",
    "\n",
    "s3text = requests.get(textURL) # get the text from the bucket\n",
    "d = {\"url\": textURL}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "bigrams = json.loads(response.text)\n",
    "\n",
    "print ('\\nprinting top 5, with POST method')\n",
    "for pair in bigrams['bigrams'][:5]:\n",
    "    print('\\n')\n",
    "    print(\"(\",pair[0][0],\",\",pair[0][1],\"), which appears\",pair[1],\" times\") # you should make this print something nicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save your notebook, download it as HTML and submit both the .ipynb and .html files to Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes about going \"Above and Beyond\"\n",
    "\n",
    "There are ample opportunities for extending this homework assignment.  You might, for example, decide to break the microservice into three separate ones (normalizing, tokenizing, and creating bigrams).  Alternatively, you might invest time into getting NLTK data into Lambda so you can use its functionality (see https://stackoverflow.com/questions/42394335/paths-in-aws-lambda-with-python-nltk).  Another interesting investigation might be to use the addition of a data file to an S3 bucket as a trigger to run the bigram analysis, perhaps writing the results to another (public) bucket.\n",
    "\n",
    "**IF YOU CHOOSE TO GO ABOVE AND BEYOND, YOU _MUST_ CHANGE THE FOLLOWING MARKDOWN BLOCK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above and Beyond\n",
    "\n",
    "Indicate here why you believe that your work should be considered \"above and beyond\".\n",
    "\n",
    "I seperated the normalizing, tokenizing, and creating bigrams into three different micro services\n",
    "\n",
    "text retrival:\n",
    "    invoke: https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/texttrtrival\n",
    "    parameter:\n",
    "        url: the url to the text file\n",
    "            example: 'https://s3.us-east-2.amazonaws.com/bucket-for-si330-hmwk4-dinganc/PrideAndPrejudice.txt'\n",
    "    response:\n",
    "        a jason string,'{'text':'text content of the file'}'\n",
    "        \n",
    "tokenization:\n",
    "    invoke: https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/tokenization\n",
    "    parameter:\n",
    "        text: a jason string containing the text for tokenization\n",
    "            example: '{'text':'however little known the feelings or views of such a man may be on his first ent it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife'}'\n",
    "    response:\n",
    "        a jason string,'{'sentences':a list of tokenized sentences}'\n",
    "        \n",
    "generate bigram:\n",
    "    invoke: https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/genbigram\n",
    "    parameter:\n",
    "        sentences: a jason string containing the list of sentences\n",
    "            example: '{'sentences':['however little known the feelings or views of such a man', 'may be on his first ent it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife']}'\n",
    "    response:\n",
    "        a list of list, which includes bigrams and frequency of apperance of that bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for text retrival\n",
    "\"\"\"\n",
    "This API takes in an url to a text file and returns the text content of the link as a json formatted string\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "from collections import defaultdict\n",
    "# normalize: convert the text to lowercase\n",
    "def get_text_and_normalize(url):\n",
    "    r=requests.get(url)\n",
    "    r.encoding='utf-8'\n",
    "    text=r.text.lower().strip()\n",
    "    text=re.sub(r'\\ufeff','',text)\n",
    "    text=re.sub(r'[^\\w\\s]','',text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"text\": \"\"}\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            url = params['url'] # retrieve the text from the URL\n",
    "            try:\n",
    "                d['text']=get_text_and_normalize(url)\n",
    "            except:\n",
    "                d['error']='error in retrieving linked text file'\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'url' in body:\n",
    "            url=body['url']\n",
    "            try:\n",
    "                d['text']=get_text_and_normalize(url)\n",
    "            except:\n",
    "                d['error']='error in retrieving linked text file'\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it is a truth universally acknowledged that a single man in possession\n",
      "of a good fortune must be in want of a wife\n",
      "\n",
      "however little known the feelings or views of such a man may be on his\n",
      "first ent\n",
      "it is a truth universally acknowledged that a single man in possession\n",
      "of a good fortune must be in want of a wife\n",
      "\n",
      "however little known the feelings or views of such a man may be on his\n",
      "first ent\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/texttrtrival' \n",
    "textURL = 'https://s3.us-east-2.amazonaws.com/bucket-for-si330-hmwk4-dinganc/PrideAndPrejudice.txt'\n",
    "\n",
    "# Demonstrate the GET functionality by passing the URL of your text file in S3 to your Lambda as a GET request\n",
    "response = requests.get(lambdaURL + '?url=' + textURL)\n",
    "text = json.loads(response.text)\n",
    "\n",
    "print (text['text'][:200])\n",
    "\n",
    "\n",
    "# Demonstrate the POST functionality by passing the text as a JSON parameter to requests.post()\n",
    "\n",
    "d = {\"url\": textURL}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "text = json.loads(response.text)\n",
    "\n",
    "print (text['text'][:200])\n",
    "\n",
    "rtr_text=response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for tokenization\n",
    "\"\"\"\n",
    "This API takes in a jason string from the above call and returns the tokenization as a json formatted string\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "# tokenize: split the text into sentences, the split each sentence into words\n",
    "def tokenize(text):\n",
    "    sentences=[re.sub(r'\\r\\n',' ',i) for i in re.split(r'\\r\\n\\r\\n',text)if i !='']\n",
    "    return sentences\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"sentences\": \"\"}\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            text = json.loads(params['text'])['text'] # retrieve the text from the JSON string\n",
    "            try:\n",
    "                d['sentences']=tokenize(text)\n",
    "            except:\n",
    "                d['error']='error in parsing linked text file'\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'text' in body:\n",
    "            text = json.loads(body['text'])['text'] # retrieve the text from the JSON string\n",
    "            try:\n",
    "                d['sentences']=tokenize(text)\n",
    "            except:\n",
    "                d['error']='error in parsing linked text file'\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"sentences\": [\"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\", \"however little known the feelings or views of such a man may be on\n",
      "{\"sentences\": [\"it is a truth universally acknowledged that a single man in possession of a good fortune must be in want of a wife\", \"however little known the feelings or views of such a man may be on\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/tokenization'\n",
    "response = requests.get(lambdaURL + '?text=' + rtr_text)\n",
    "\n",
    "print (response.text[:200])\n",
    "\n",
    "d = {'text':rtr_text}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "\n",
    "print (response.text[:200])\n",
    "rtr_sent=response.text\n",
    "# you should make this print something nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code for generating bigram count\n",
    "\"\"\"\n",
    "This API takes in a json formatted sentence list and returns the ngram count as a json formatted string\n",
    "\"\"\"\n",
    "import json\n",
    "import re\n",
    "from botocore.vendored import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "# find bigrams\n",
    "# NOTE: it's very difficult to set up NLTK on Lambda, so you'll need to find bigrams \"manually\"\n",
    "# NOTE: see https://stackoverflow.com/questions/21844546/forming-bigrams-of-words-in-list-of-sentences-with-python\n",
    "#       for hints about how to create bigrams\n",
    "    \n",
    "# count bigrams\n",
    "    \n",
    "# Note the strict format of the return dictionary\n",
    "# It must contain these three elements, and the body\n",
    "# must be a stringified JSON object (i.e. you have to call \n",
    "# json.dumps on the JSON structure you're returning)\n",
    "\n",
    "def bigram_count(sentences):\n",
    "    bigrams=[]\n",
    "    bigram_k=defaultdict(int)\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        for i in range(len(words)):\n",
    "            if i < len(words) - 1:\n",
    "                bigrams.append((words[i], words[i + 1]))\n",
    "    for bigram in bigrams:\n",
    "        bigram_k[bigram]+=1\n",
    "    return sorted(bigram_k.items(),key=lambda x: x[1],reverse=True)\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    method = event['httpMethod']\n",
    "    text = \"\"\n",
    "    d = {\"bigrams\": \"\"}\n",
    "    if method == 'GET':\n",
    "        params = event['queryStringParameters']\n",
    "        if params:\n",
    "            sentences = json.loads(params['sentences'])['sentences'] # retrieve the token list from the json string\n",
    "            try:\n",
    "                d['bigrams']=bigram_count(sentences)\n",
    "            except:\n",
    "                d['error']='error in generating bigram from provided token list'\n",
    "    if method == 'POST':\n",
    "        body = json.loads(event['body'])\n",
    "        if 'sentences' in body:\n",
    "            sentences = json.loads(body['sentences'])['sentences'] # retrieve the token list from the json string\n",
    "            try:\n",
    "                d['bigrams']=bigram_count(sentences)\n",
    "            except:\n",
    "                d['error']='error in generating bigram from provided token list'\n",
    "    return { \n",
    "        \"statusCode\": 200,\n",
    "        \"headers\": {\"Content-Type\": \"application/json\"},\n",
    "        \"body\": json.dumps(d),\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "printing top 5, with GET method\n",
      "\n",
      "\n",
      "( that , he ), which appears 5  times\n",
      "\n",
      "\n",
      "( my , dear ), which appears 4  times\n",
      "\n",
      "\n",
      "( mr , bennet ), which appears 4  times\n",
      "\n",
      "\n",
      "( it , is ), which appears 2  times\n",
      "\n",
      "\n",
      "( a , single ), which appears 2  times\n",
      "\n",
      "printing top 5, with POST method\n",
      "\n",
      "\n",
      "( that , he ), which appears 5  times\n",
      "\n",
      "\n",
      "( my , dear ), which appears 4  times\n",
      "\n",
      "\n",
      "( mr , bennet ), which appears 4  times\n",
      "\n",
      "\n",
      "( it , is ), which appears 2  times\n",
      "\n",
      "\n",
      "( a , single ), which appears 2  times\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "lambdaURL = 'https://vnbny82y4k.execute-api.us-east-1.amazonaws.com/prod/genbigram' # change this URL\n",
    "response = requests.get(lambdaURL + '?sentences=' + rtr_sent)\n",
    "bigrams = json.loads(response.text)\n",
    "\n",
    "print ('\\nprinting top 5, with GET method')\n",
    "for pair in bigrams['bigrams'][:5]:\n",
    "    print('\\n')\n",
    "    print(\"(\",pair[0][0],\",\",pair[0][1],\"), which appears\",pair[1],\" times\") \n",
    "\n",
    "d = {'sentences':rtr_sent}\n",
    "response = requests.post(lambdaURL, json = d)\n",
    "bigrams = json.loads(response.text)\n",
    "\n",
    "print ('\\nprinting top 5, with POST method')\n",
    "for pair in bigrams['bigrams'][:5]:\n",
    "    print('\\n')\n",
    "    print(\"(\",pair[0][0],\",\",pair[0][1],\"), which appears\",pair[1],\" times\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
